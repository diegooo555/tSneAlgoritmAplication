# üî¨ Interactive t-SNE Explorer: Dimensionality Reduction

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-1.24%2B-red)
![Status](https://img.shields.io/badge/Status-Active-success)

## üìñ Descripci√≥n del Proyecto

Esta aplicaci√≥n es una herramienta interactiva de **Machine Learning** dise√±ada para visualizar la reducci√≥n de dimensionalidad en datos complejos. Utiliza el algoritmo **t-SNE** (t-Distributed Stochastic Neighbor Embedding) sobre el dataset cl√°sico de d√≠gitos manuscritos (UCI ML Digits).

El objetivo es permitir a usuarios t√©cnicos y no t√©cnicos explorar c√≥mo los algoritmos de aprendizaje de variedades transforman un espacio de 64 dimensiones en un plano 2D observable, facilitando la identificaci√≥n de cl√∫steres y patrones ocultos.

---

## üß† El Algoritmo: t-SNE

**t-SNE** es una t√©cnica de aprendizaje no supervisado no lineal. A diferencia de PCA (que es lineal), t-SNE es excelente para visualizar estructuras de alta dimensi√≥n preservando la topolog√≠a local.

### Fundamento Matem√°tico
1.  **Similitud en Alta Dimensi√≥n ($P$):** Se calcula la probabilidad condicional de similitud entre puntos basada en una distribuci√≥n Gaussiana.
2.  **Similitud en Baja Dimensi√≥n ($Q$):** Se utiliza una distribuci√≥n **t-Student** (de colas pesadas) en el espacio 2D. Esto mitiga el "problema de aglomeraci√≥n" (crowding problem).
3.  **Funci√≥n de Coste:** El algoritmo minimiza la Divergencia de Kullback-Leibler (KL) entre ambas distribuciones mediante Gradiente Descendente:

$$KL(P||Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

---

## üìä El Dataset: Digits

Utilizamos el dataset `sklearn.datasets.load_digits`.

* **Muestras:** 1,797 vectores.
* **Dimensionalidad:** 64 features (im√°genes aplanadas de 8x8 p√≠xeles).
* **Clases:** 10 (D√≠gitos 0-9).
* **Valores:** Enteros de 0 a 16 (escala de grises).

---

## üõ†Ô∏è Stack Tecnol√≥gico

| Componente | Librer√≠a | Prop√≥sito |
| :--- | :--- | :--- |
| **Frontend** | `Streamlit` | Interfaz de usuario reactiva y widgets de control. |
| **ML Core** | `Scikit-Learn` | Implementaci√≥n de t-SNE y carga de datos. |
| **Data Frame** | `Pandas` | Manipulaci√≥n y estructuraci√≥n de proyecciones. |
| **Visualizaci√≥n** | `Plotly Express` | Gr√°ficos de dispersi√≥n interactivos (zooming, tooltips). |
| **Renderizado** | `Matplotlib` | Visualizaci√≥n est√°tica de las matrices de im√°genes crudas. |

---

## üèó Arquitectura del Sistema

El proyecto sigue el principio de dise√±o **Separation of Concerns (SoC)**. La l√≥gica de negocio, la ingesti√≥n de datos y la visualizaci√≥n est√°n desacopladas, permitiendo modificar componentes individuales sin afectar la estabilidad del sistema completo.

### Diagrama de Flujo de Datos

```mermaid
graph LR
    A[Usuario / GUI] -->|Configuraci√≥n| B(app.py - Orquestador)
    B -->|Solicitud Datos| C[src/data_loader.py]
    C -->|Raw Data| B
    B -->|Datos + Par√°metros| D[src/model_engine.py]
    D -->|Procesamiento t-SNE| D
    D -->|DataFrame Proyectado| B
    B -->|Resultados| E[src/visualization.py]
    E -->|Objetos Gr√°ficos| B
    B -->|Renderizado| A
```
---

## üöÄ Instalaci√≥n y Uso

Sigue estos pasos para ejecutar el proyecto en tu entorno local:

### 1. Clonar el repositorio
```bash
git clone [https://github.com/TU_USUARIO/tsne-explorer.git](https://github.com/TU_USUARIO/tsne-explorer.git)
cd tsne-explorer
```

### 2. Crear Entorno Virtual
```bash
python -m venv venv
source venv/bin/activate  
venv\Scripts\activate
```
### 3. Instalar Dependencias
```bash
pip install -r requirements.txt
```

---
### ‚öôÔ∏è Explicaci√≥n de Par√°metros del Algoritmo

El algoritmo **t-SNE** es sensible a sus hiperpar√°metros. A continuaci√≥n se detalla qu√© controla cada variable configurada en la barra lateral:

#### 1. Perplexity (Perplejidad)
**Rango seleccionado:** `5 - 50`

Es, quiz√°s, el par√°metro m√°s importante. Matem√°ticamente, es una medida del n√∫mero efectivo de vecinos cercanos que cada punto considera.
* **¬øQu√© hace?** Controla el equilibrio entre prestar atenci√≥n a los aspectos **locales** de los datos frente a los **globales**.
* **Perplexity Baja (5-10):** El algoritmo se enfoca solo en los vecinos inmediatos. Esto tiende a romper los cl√∫steres en grupos peque√±os y fragmentados.
* **Perplexity Alta (30-50):** El algoritmo considera un vecindario m√°s amplio. Preserva mejor la topolog√≠a global, pero puede fusionar grupos que deber√≠an estar separados.
* *Analog√≠a:* Es como ajustar el "zoom" de una c√°mara. ¬øQuieres ver los detalles de una sola hoja (baja) o el bosque completo (alta)?

#### 2. Iteraciones (n_iter)
**Rango seleccionado:** `250 - 2000`

Define el n√∫mero m√°ximo de pasos que el algoritmo ejecutar√° para optimizar la posici√≥n de los puntos.
* **¬øQu√© hace?** t-SNE comienza con los puntos en posiciones aleatorias y los mueve paso a paso para minimizar el error (Divergencia KL).
* **Pocas iteraciones (< 250):** El modelo puede detenerse antes de converger. Los resultados parecer√°n una "bola" de puntos desorganizada sin cl√∫steres definidos.
* **Muchas iteraciones (> 1000):** Asegura que el modelo encuentre una configuraci√≥n estable. Una vez que se alcanza la estabilidad, m√°s iteraciones no cambian el resultado, solo consumen tiempo de c√≥mputo.

#### 3. Learning Rate (Tasa de Aprendizaje)
**Valor seleccionado:** `100` (u opciones: 10, 50, 200)

Controla el tama√±o del "paso" que da el algoritmo en cada actualizaci√≥n durante la optimizaci√≥n del gradiente descendente.
* **¬øQu√© hace?** Determina qu√© tan r√°pido se mueven los puntos hacia su posici√≥n ideal en cada iteraci√≥n.
* **Tasa Baja (10):** Los puntos se mueven muy lentamente. Puede requerir much√≠simas iteraciones para llegar a una soluci√≥n y corre el riesgo de quedarse atascado en √≥ptimos locales.
* **Tasa Alta (200+):** Los puntos se mueven agresivamente. Pueden "saltarse" la posici√≥n ideal, resultando en una visualizaci√≥n donde todos los puntos parecen equidistantes y dispersos (como una nube uniforme).
* **Valor T√≠pico:** Para datasets de este tama√±o, valores entre `100` y `200` suelen ofrecer el mejor equilibrio.